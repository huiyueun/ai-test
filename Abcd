import gradio as gr
from llama_cpp import Llama
import os
import sys

# --- ì„¤ì •ê°’ ë¶ˆëŸ¬ì˜¤ê¸° ---
sys.path.append(os.path.dirname(os.path.dirname(__file__)))
from config import save_dir_merged

GGUF_MODEL_PATH = os.path.join(save_dir_merged, "model.gguf")
MAX_TOKENS = 256

# --- ëª¨ë¸ ë¡œë“œ ---
print("ğŸ”„ ëª¨ë¸ ë¡œë”© ì¤‘...")
llm = Llama(
    model_path=GGUF_MODEL_PATH,
    n_ctx=2048,
    n_threads=8,         # CPU ìŠ¤ë ˆë“œ ìˆ˜
    n_batch=256,
    n_gpu_layers=20      # GPU ì‚¬ìš©ì‹œ ì ì ˆíˆ ì¡°ì • (ì—†ìœ¼ë©´ 0)
)
print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ")

# --- ìš”ì•½ ìƒì„± í•¨ìˆ˜ ---
def generate_summary(screen_json: str) -> str:
    if not screen_json.strip():
        return "â— JSON ì…ë ¥ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤."

    if "qwen" in save_dir_merged:
        prompt = f"<|im_start|>user\në‹¤ìŒ í™”ë©´ì„ í•œêµ­ì–´ë¡œ ìš”ì•½í•´ì¤˜:\n{screen_json}\n<|im_end|>\n<|im_start|>assistant"
    else:
        prompt = f"<bos><start_of_turn>user\në‹¤ìŒ í™”ë©´ì„ í•œêµ­ì–´ë¡œ ìš”ì•½í•´ì¤˜:\n{screen_json}\n<end_of_turn>\n<start_of_turn>model"

    response = llm(
        prompt,
        max_tokens=MAX_TOKENS,
        temperature=1.0,
        top_p=0.95,
        top_k=64,
        stop=["<|im_end|>", "<end_of_turn>"]
    )
    return response["choices"][0]["text"].strip()

# --- Gradio UI ---
with gr.Blocks() as demo:
    gr.Markdown("# ğŸ§  LLaMA GGUF ìš”ì•½ê¸°")
    gr.Markdown("ì…ë ¥ëœ UI í™”ë©´ êµ¬ì¡°(JSON)ë¥¼ í•œêµ­ì–´ë¡œ ìš”ì•½í•©ë‹ˆë‹¤.")

    with gr.Row():
        input_json = gr.Textbox(label="ğŸ“¥ í™”ë©´ JSON ì…ë ¥", lines=20, placeholder="ì—¬ê¸°ì— JSONì„ ë¶™ì—¬ë„£ìœ¼ì„¸ìš”")
        output_text = gr.Textbox(label="ğŸ“¤ ìš”ì•½ ê²°ê³¼", lines=20)

    run_button = gr.Button("ğŸ“ ìš”ì•½ ì‹¤í–‰")
    run_button.click(fn=generate_summary, inputs=input_json, outputs=output_text)

# ì‹¤í–‰
if __name__ == "__main__":
    demo.launch()
