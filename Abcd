import gradio as gr
import subprocess
import os
import sys

# ê²½ë¡œ ì„¤ì •
sys.path.append(os.path.dirname(os.path.dirname(__file__)))
from config import llama_cpp_path, save_dir_merged

LLAMA_CLI_BIN = llama_cpp_path + "/build/bin/llama-cli"
GGUF_MODEL_PATH = save_dir_merged + "/model.gguf"
MAX_TOKENS = 256

# llama-cli ì‹¤í–‰ í•¨ìˆ˜
def generate_summary(screen_json):
    if not screen_json.strip():
        return "ì…ë ¥ê°’ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤."

    if "qwen" in save_dir_merged:
        prompt = f"<|im_start|>user\nGenerate a summary for the following screen in Korean:\n{screen_json}\n<|im_end|>\n<|im_start|>assistant"
    else:
        prompt = f"<bos><start_of_turn>user\nGenerate a summary for the following screen in Korean:\n{screen_json}\n<end_of_turn>\n<start_of_turn>model"

    command = [
        LLAMA_CLI_BIN,
        "-m", GGUF_MODEL_PATH,
        "-p", prompt,
        "--n-predict", str(MAX_TOKENS),
        "--temp", "1.0",
        "--top-p", "0.95",
        "--top-k", "64"
    ]

    try:
        process = subprocess.Popen(
            command,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            text=True,
            bufsize=1
        )

        result_lines = []
        for line in process.stdout:
            result_lines.append(line.strip())

        process.wait()
        return "\n".join(result_lines)

    except Exception as e:
        return f"ì—ëŸ¬ ë°œìƒ: {str(e)}"

# Gradio UI ì„¤ì •
with gr.Blocks() as demo:
    gr.Markdown("# ğŸ§  LLaMA GGUF ìš”ì•½ê¸°\nJSON í™”ë©´ êµ¬ì¡°ë¥¼ ë„£ê³  ìš”ì•½ ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”.")

    with gr.Row():
        input_json = gr.Textbox(lines=20, label="ğŸ“¥ í™”ë©´ JSON ì…ë ¥")
        output_text = gr.Textbox(lines=20, label="ğŸ“¤ ìš”ì•½ ê²°ê³¼")

    run_button = gr.Button("ìš”ì•½ ì‹¤í–‰")

    run_button.click(fn=generate_summary, inputs=input_json, outputs=output_text)

# ì•± ì‹¤í–‰
if __name__ == "__main__":
    demo.launch()
