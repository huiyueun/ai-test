from unsloth import FastLanguageModel
from unsloth.chat_templates import get_chat_template
from trl import SFTTrainer
from datasets import load_dataset
import torch
import json
import os
import sys
sys.path.append(os.path.dirname(os.path.dirname(__file__)))

from config import model_name, max_seq_length, save_dir_lora
from shutil import copytree

os.environ["CUDA_VISIBLE_DEVICES"] = "0"
os.environ["TOKENIZERS_PARALLELISM"] = "false"


# === 모델 로드 ===
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = model_name,
    max_seq_length = max_seq_length,
    dtype = torch.float16,
    load_in_4bit = True,
)

if "Qwen3" in model_name:
    model = FastLanguageModel.get_peft_model(
        model,
        r = 8,
        lora_alpha = 16,
        lora_dropout = 0.05,
        bias = "none",
        random_state = 3407,
    )
else:
    model = FastLanguageModel.get_peft_model(
        model,
        finetune_vision_layers = False,
        finetune_language_layers = True,
        finetune_attention_modules = True,
        finetune_mlp_modules = True,
        use_gradient_checkingpointing = True,
        r = 8,
        lora_alpha = 16,
        lora_dropout = 0.05,
        bias = "none",
        random_state = 3407,
    )

FastLanguageModel.for_training(model)  # prepare for SFT

project_root = os.path.dirname(os.path.dirname(__file__))
# === 데이터 로드 ===
dataset = load_dataset("json", data_files={
    "train": os.path.join(project_root,"sample_data/train_oversampled.jsonl"),
    "eval": os.path.join(project_root,"sample_data/eval_oversampled.jsonl")
})

# === 전처리 함수 (멀티태스크 구성: short, full, Q&A) ===
def format_sample(example):
    #print(example)
    messages = []
    if "input" in example:
        screen = example["input"]
    else:
        screen = "{}"

    # if "short" in example and example["short"]:
    #     messages.append({
    #         "role": "user",
    #         "content": f"SHORT_SUMMARY: Write a short title for this screen in Korean:\n{screen}"
    #     })
    #     messages.append({"role": "assistant", "content": example["short"]})
    if "full" in example and example["full"]:
        messages.append({
            "role": "user",
            "content": f"Generate a summary for the following screen in Korean:\n{screen}"
        })
        if "Qwen3" in model_name:
            messages.append({"role": "assistant", "content": example["full"]})
        else:
            messages.append({"role": "model", "content": example["full"]})
    # if "qa" in example and example["qa"]:
    #     for qa in example["qa"]:
    #         messages.append({"role": "user", "content": qa["question"]})
    #         messages.append({"role": "model", "content": qa["answer"]})
    return tokenizer.apply_chat_template(messages, tokenize=False)

#=== Tokenize dataset ===
if "Qwen3" in model_name:
    tokenizer.padding_side = "left" # for qwen3
tokenized = dataset.map(lambda x: {"text": format_sample(x)})
print(tokenized["train"][10]["text"])  # check the result

# === SFTTrainer ===
from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir = "unsloth-multitask-output",
    num_train_epochs = 3,
    per_device_train_batch_size = 4,
    gradient_accumulation_steps = 4,
    learning_rate = 2e-5,
    logging_dir = "./logs",
    logging_steps = 25,
    save_strategy = "epoch",       # only "no" or "epoch"
    save_total_limit = 2,
    optim = "adamw_8bit",
    fp16 = True,
    bf16 = False,
    report_to = "tensorboard",
)

trainer = SFTTrainer(
    model = model,
    tokenizer = tokenizer,
    train_dataset = tokenized["train"],
    eval_dataset = tokenized["eval"],
    args = training_args,
    dataset_text_field = "text",
    max_seq_length = 2048,
    packing = False,
)
trainer.train()

model.save_pretrained(save_dir_lora)
tokenizer.save_pretrained(save_dir_lora)

if not os.path.exists("../unsloth_test/models/Qwen3-1.7B-unsloth-bnb-4bit"):
    copytree("unsloth-multitask-output", "../unsloth_test/models/Qwen3-1.7B-unsloth-bnb-4bit")
