from unsloth import FastLanguageModel
from unsloth.chat_templates import get_chat_template
from trl import SFTTrainer
from datasets import load_dataset
import torch
import json
import os
from config import model_name, max_seq_length, save_dir_lora

os.environ["CUDA_VISIBLE_DEVICES"] = "0"
os.environ["TOKENIZERS_PARALLELISM"] = "false"


# === 모델 로드 ===
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = model_name,
    max_seq_length = max_seq_length,
    dtype = torch.float16,
    load_in_4bit = True,
)

if "Qwen3" in model_name:
    model = FastLanguageModel.get_peft_model(
        model,
        r = 8,
        lora_alpha = 16,
        lora_dropout = 0.05,
        bias = "none",
        random_state = 3407,
    )
else:
    model = FastLanguageModel.get_peft_model(
        model,
        finetune_vision_layers = False,
        finetune_language_layers = True,
        finetune_attention_modules = True,
        finetune_mlp_modules = True,
        use_gradient_checkingpointing = True,
        r = 8,
        lora_alpha = 16,
        lora_dropout = 0.05,
        bias = "none",
        random_state = 3407,
    )

FastLanguageModel.for_training(model)  # prepare for SFT

# === 데이터 로드 ===
dataset = load_dataset("json", data_files={
    "train": "train_oversampled.jsonl",
    "eval": "eval_oversampled.jsonl"
})

# === 전처리 함수 (멀티태스크 구성: short, full, Q&A) ===
def format_sample(example):
    #print(example)
    messages = []
    if "input" in example:
        screen = example["input"]
    else:
        screen = "{}"

    # if "short" in example and example["short"]:
    #     messages.append({
    #         "role": "user",
    #         "content": f"SHORT_SUMMARY: Write a short title for this screen in Korean:\n{screen}"
    #     })
    #     messages.append({"role": "assistant", "content": example["short"]})
    if "full" in example and example["full"]:
        messages.append({
            "role": "user",
            "content": f"Generate a summary for the following screen in Korean:\n{screen}"
        })
        if "Qwen3" in model_name:
            messages.append({"role": "assistant", "content": example["full"]})
        else:
            messages.append({"role": "model", "content": example["full"]})
    # if "qa" in example and example["qa"]:
    #     for qa in example["qa"]:
    #         messages.append({"role": "user", "content": qa["question"]})
    #         messages.append({"role": "model", "content": qa["answer"]})
    return tokenizer.apply_chat_template(messages, tokenize=False)

#=== Tokenize dataset ===
if "Qwen3" in model_name:
    tokenizer.padding_side = "left" # for qwen3
tokenized = dataset.map(lambda x: {"text": format_sample(x)})
print(tokenized["train"][10]["text"])  # check the result

# === SFTTrainer ===
trainer = SFTTrainer(
    model = model,
    train_dataset = tokenized["train"],
    eval_dataset = tokenized["eval"],
    tokenizer = tokenizer,
    max_seq_length = 2048,
    dataset_text_field = "text",
    packing = False,
    args = dict(
        per_device_train_batch_size = 4,
        gradient_accumulation_steps = 4,
        num_train_epochs = 3,
        evaluation_strategy = "steps",
        save_strategy = "epoch",
        learning_rate = 2e-5,
        save_total_limit = 2,
        output_dir = "unsloth-multitask-output",
        optim = "adamw_8bit",
        bf16 = torch.cuda.is_bf16_supported(),
        logging_dir = "./logs",
        report_to = "tensorboard",
        logging_steps = 25,
    )
)

trainer.train()

model.save_pretrained(save_dir_lora)
tokenizer.save_pretrained(save_dir_lora)
