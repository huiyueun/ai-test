from unsloth import FastLanguageModel
from transformers import TextStreamer
import torch
import json
import os
import sys
sys.path.append(os.path.dirname(os.path.dirname(__file__)))

# === 경로 지정 ===
base_model_path = "unsloth/Qwen3-1.7B-unsloth-bnb-4bit"  # 사전학습모델
lora_path = "output/lora_qwen"  # LoRA 결과 저장 폴더


# === 모델 로드 ===
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name=lora_path,
    max_seq_length=2048,
    dtype=torch.float16,
    load_in_4bit=True,
)

# === 평가 모드 전환 ===
model.eval()

# === 테스트용 입력 ===
sample_input = {
  "list:top": [
    "\"리모컨 없이 호출하기\",btn:\"꺼짐\""
  ],
  "list:mid": [
    "\"목소리 등록\",btn:\"등록\""
  ]
}

# === 메시지 포맷 구성 ===
messages = [
    {
        "role": "user",
        "content": f"다음 UI 화면을 요약해줘 (한국어):\n{json.dumps(sample_input, ensure_ascii=False)}"
    }
]

# === Chat 템플릿 적용 ===
input_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

# === 생성 ===
inputs = tokenizer(input_text, return_tensors="pt").to(model.device)
with torch.no_grad():
    outputs = model.generate(
        **inputs,
        max_new_tokens=128,
        do_sample=False,
        temperature=0.7,
    )

# === 출력 ===
print(tokenizer.decode(outputs[0], skip_special_tokens=True))


(unsloth_env) huiyu@huiyu-linux2:~/Workspace/AIAssistant/AIAssistantTraining$ python3 local_test/inference_test.py 
🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.
🦥 Unsloth Zoo will now patch everything to make training faster!
==((====))==  Unsloth 2025.6.12: Fast Qwen3 patching. Transformers: 4.53.0.
   \\   /|    NVIDIA GeForce RTX 3090 Ti. Num GPUs = 1. Max memory: 23.525 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.7.1+cu126. CUDA: 8.6. CUDA Toolkit: 12.6. Triton: 3.3.1
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.31. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth 2025.6.12 patched 28 layers with 0 QKV layers, 0 O layers and 0 MLP layers.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Qwen3ForCausalLM has no `_prepare_4d_causal_attention_mask_with_cache_position` method defined in its base modeling class. Compiled forward passes will be sub-optimal. If you're writing code, see Llama for an example implementation. If you're a user, please report this issue on GitHub.
user
다음 UI 화면을 요약해줘 (한국어):
{"list:top": ["\"리모컨 없이 호출하기\",btn:\"꺼짐\""], "list:mid": ["\"목소리 등록\",btn:\"등록\""]}
assistant
<think>
Okay, let's see. The user wants me to summarize a UI screen in Korean. The input is a JSON structure with "list:top" and "list:mid". 

First, I need to parse the JSON. The "list:top" has two items. The first is "리모컨 없이 호출하기" with a button "꺼짐". The second is "목소리 등록" with "btn:\"등록\"". Wait, the JSON shows the buttons as "btn:\"등록\"", but in the example, it's written as "btn:\"등록\"". Maybe the
