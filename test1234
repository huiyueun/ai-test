from unsloth import FastLanguageModel
from transformers import TextStreamer
import torch
import json
import os
import sys
sys.path.append(os.path.dirname(os.path.dirname(__file__)))

# === ê²½ë¡œ ì§€ì • ===
base_model_path = "unsloth/Qwen3-1.7B-unsloth-bnb-4bit"  # ì‚¬ì „í•™ìŠµëª¨ë¸
lora_path = "output/lora_qwen"  # LoRA ê²°ê³¼ ì €ì¥ í´ë”


# === ëª¨ë¸ ë¡œë“œ ===
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name=lora_path,
    max_seq_length=2048,
    dtype=torch.float16,
    load_in_4bit=True,
)

# === í‰ê°€ ëª¨ë“œ ì „í™˜ ===
model.eval()

# === í…ŒìŠ¤íŠ¸ìš© ì…ë ¥ ===
sample_input = {
  "list:top": [
    "\"ë¦¬ëª¨ì»¨ ì—†ì´ í˜¸ì¶œí•˜ê¸°\",btn:\"êº¼ì§\""
  ],
  "list:mid": [
    "\"ëª©ì†Œë¦¬ ë“±ë¡\",btn:\"ë“±ë¡\""
  ]
}

# === ë©”ì‹œì§€ í¬ë§· êµ¬ì„± ===
messages = [
    {
        "role": "user",
        "content": f"ë‹¤ìŒ UI í™”ë©´ì„ ìš”ì•½í•´ì¤˜ (í•œêµ­ì–´):\n{json.dumps(sample_input, ensure_ascii=False)}"
    }
]

# === Chat í…œí”Œë¦¿ ì ìš© ===
input_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

# === ìƒì„± ===
inputs = tokenizer(input_text, return_tensors="pt").to(model.device)
with torch.no_grad():
    outputs = model.generate(
        **inputs,
        max_new_tokens=128,
        do_sample=False,
        temperature=0.7,
    )

# === ì¶œë ¥ ===
print(tokenizer.decode(outputs[0], skip_special_tokens=True))


(unsloth_env) huiyu@huiyu-linux2:~/Workspace/AIAssistant/AIAssistantTraining$ python3 local_test/inference_test.py 
ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.
ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!
==((====))==  Unsloth 2025.6.12: Fast Qwen3 patching. Transformers: 4.53.0.
   \\   /|    NVIDIA GeForce RTX 3090 Ti. Num GPUs = 1. Max memory: 23.525 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.7.1+cu126. CUDA: 8.6. CUDA Toolkit: 12.6. Triton: 3.3.1
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.31. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth 2025.6.12 patched 28 layers with 0 QKV layers, 0 O layers and 0 MLP layers.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Qwen3ForCausalLM has no `_prepare_4d_causal_attention_mask_with_cache_position` method defined in its base modeling class. Compiled forward passes will be sub-optimal. If you're writing code, see Llama for an example implementation. If you're a user, please report this issue on GitHub.
user
ë‹¤ìŒ UI í™”ë©´ì„ ìš”ì•½í•´ì¤˜ (í•œêµ­ì–´):
{"list:top": ["\"ë¦¬ëª¨ì»¨ ì—†ì´ í˜¸ì¶œí•˜ê¸°\",btn:\"êº¼ì§\""], "list:mid": ["\"ëª©ì†Œë¦¬ ë“±ë¡\",btn:\"ë“±ë¡\""]}
assistant
<think>
Okay, let's see. The user wants me to summarize a UI screen in Korean. The input is a JSON structure with "list:top" and "list:mid". 

First, I need to parse the JSON. The "list:top" has two items. The first is "ë¦¬ëª¨ì»¨ ì—†ì´ í˜¸ì¶œí•˜ê¸°" with a button "êº¼ì§". The second is "ëª©ì†Œë¦¬ ë“±ë¡" with "btn:\"ë“±ë¡\"". Wait, the JSON shows the buttons as "btn:\"ë“±ë¡\"", but in the example, it's written as "btn:\"ë“±ë¡\"". Maybe the
