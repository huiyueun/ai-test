ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.
ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!
==((====))==  Unsloth 2025.6.12: Fast Qwen3 patching. Transformers: 4.53.0.
   \\   /|    NVIDIA GeForce RTX 3090 Ti. Num GPUs = 1. Max memory: 23.525 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.7.1+cu126. CUDA: 8.6. CUDA Toolkit: 12.6. Triton: 3.3.1
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.31. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Traceback (most recent call last):
  File "/home/huiyu/Workspace/AIAssistant/AIAssistantTraining/local_test/inference_test.py", line 23, in <module>
    model = FastLanguageModel.get_peft_model(model, lora_path)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/huiyu/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/unsloth/models/llama.py", line 2199, in get_peft_model
    raise TypeError(f"Unsloth: Rank of {str(r)} must be an integer.")
TypeError: Unsloth: Rank of output/lora_qwen must be an integer.
