from unsloth import FastLanguageModel
from unsloth.chat_templates import get_chat_template
from trl import SFTTrainer
from datasets import load_dataset
import torch
import os
import sys

from transformers import TrainingArguments

# === 경로 설정 ===
sys.path.append(os.path.dirname(os.path.dirname(__file__)))
from config import model_name, max_seq_length, save_dir_lora

os.environ["CUDA_VISIBLE_DEVICES"] = "0"
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# === 모델 로드 ===
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name=model_name,
    max_seq_length=max_seq_length,
    dtype=torch.float16,
    load_in_4bit=True,
)

if "Qwen3" in model_name:
    model = FastLanguageModel.get_peft_model(
        model,
        r=8,
        lora_alpha=16,
        lora_dropout=0.05,
        bias="none",
        random_state=3407,
    )
else:
    model = FastLanguageModel.get_peft_model(
        model,
        finetune_vision_layers=False,
        finetune_language_layers=True,
        finetune_attention_modules=True,
        finetune_mlp_modules=True,
        use_gradient_checkingpointing=True,
        r=8,
        lora_alpha=16,
        lora_dropout=0.05,
        bias="none",
        random_state=3407,
    )

FastLanguageModel.for_training(model)

# === 데이터 로드 ===
project_root = os.path.dirname(os.path.dirname(__file__))
train_dataset = load_dataset("json", data_files={
    "train": os.path.join(project_root, "sample_data/full_summaries.jsonl")
}, split="train")

eval_dataset = load_dataset("json", data_files={
    "eval": os.path.join(project_root, "sample_data/screen_results_deduped.jsonl")
}, split="eval")

# === 전처리 ===
def format_sample(example):
    screen = example.get("input", "")
    full = example.get("full_summary", "")

    if not screen.strip() or not full.strip():
        return {"text": None}

    messages = [
        {"role": "user", "content": f"다음 화면에 대한 요약을 생성하세요:\n{screen}"},
        {"role": "assistant", "content": full}
    ]
    return {"text": tokenizer.apply_chat_template(messages, tokenize=False)}

def is_valid(example):
    return isinstance(example.get("text"), str) and len(example["text"].strip()) > 10

# === 토큰화 ===
if "Qwen3" in model_name:
    tokenizer.padding_side = "left"

tokenized_train = train_dataset.map(format_sample)
tokenized_eval = eval_dataset.map(format_sample)

tokenized_train = tokenized_train.filter(is_valid)
tokenized_eval = tokenized_eval.filter(is_valid)

# === Eval이 비어있을 경우 처리 ===
eval_dataset_final = tokenized_eval if len(tokenized_eval) > 0 else None

# === TrainingArguments ===
training_args = TrainingArguments(
    output_dir="unsloth-multitask-output",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    gradient_accum
